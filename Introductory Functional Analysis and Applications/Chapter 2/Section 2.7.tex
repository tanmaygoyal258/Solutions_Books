\subsection{Bounded and Continuous Linear Operators}

\begin{question}
    Prove (7).
    \label{section2.7-1}
\end{question}
\begin{proof}
    We wish to show that $\norm{T_1T_2} \leq \norm{T_1}\norm{T_2}$ and $\norm{T^n} \leq \norm{T}^n$. The second follows from repeatedly applying the first. We show the first one now. Let $T_1 : Z \mapsto Y$ and $T_2 : X \mapsto Z$, then $T_1T_2 : X \mapsto Y$. We have
    \[\norm{T_1T_2} = \sup_{\norm{x} = 1} \norm{T_1T_2x} \leq \sup_{\norm{x} = 1} \norm{T_1}\norm{T_2x} =\norm{T_1}\sup_{\norm{x} = 1}\norm{T_2x} = \norm{T_1}\norm{T_2}.\]
\end{proof}

\begin{question}
    Let $X$ and $Y$ be normed spaces. Show that a linear operator $T:X \mapsto Y$ is bounded if and only if $T$ maps bounded sets in $X$ to bounded sets in $Y$.
    \label{section2.7-2}
\end{question}
\begin{proof}
    Suppose $T$ is bounded. Then,
    \[\norm{Tx} \leq c \norm{x}.\]
    Suppose, $M \subset X$ is a bounded set such that for all $x_1 , x_2 \in M, $
    \[\norm{x_1 - x_2} \leq \delta(M).\]
    Then, 
    \[\norm{Tx_1 - Tx_2} = \norm{T(x_1 - x_2)} \leq c \norm{x_1 - x_2} = c\delta(M).\]
    In other words, the set that $M$ is mapped into is also bounded.

    Now, consider bounded sets in $X$ are mapped into bounded sets in $Y$. Let $ M \subset X$ be a bounded set that maps into $N \subset Y$ and for some $x_1 ,x_2 , x \in M$ and $\epsilon \g 0$, define
    \[x_1 = x_2 + (\delta(M) - \epsilon)\frac{x}{\norm{x}} \implies \norm{x_1 - x_2} = \abs{\delta(M) - \epsilon} \leq \delta(M).\]
    Then, we have
    \[\norm{Tx_1 - Tx_2} = \norm{T(\delta(M) - \epsilon)\frac{x}{\norm{x}}} = \abs{\delta(M) - \epsilon} \frac{\norm{Tx}}{\norm{x}} \leq \delta(N) \implies \norm{Tx} \leq \frac{\delta(N)}{\abs{\delta(M) - \epsilon}} \norm{x}\]
    and hence, $T$ is bounded.
\end{proof}

\begin{question}
    If $T \neq 0$ is a bounded linear operator, show that for any $x \in \dom{T}$ such that $\norm{x} \l 1$, we have the strict inequality, $\norm{Tx} \l \norm{T}$.
    \label{section2.7-3}
\end{question}
\begin{proof}
    We have,
    \[\norm{Tx} \leq \norm{T} \norm{x} \l 1.\]
\end{proof}

\begin{question}
    Give a direct proof of 2.7-9(b), without using 2.7-9(a).
    \label{section2.7-4}
\end{question}
\begin{proof}
    We have to show that if a bounded linear operator $T$ is continuous at one point, say $x_0$, then, it is continuous throughout it's domain. Since $T$ is continuous at $x_0$, we have
    \[\norm{x_0 - x} \l \delta \implies \norm{Tx_0 - Tx} \l \epsilon.\]
    Also, since $T$ is bounded, we have
    \[\norm{Tx} \leq c \norm{x}.\]
    Now, suppose we have to test continuity at some point $x_1$ such that $\norm{x_1 - x_0} \l \delta^\prime.$ Then, we have
    \[\norm{x_1 - x} = \norm{x_1 - x_0 + x_0 - x} \leq \norm{x_1 - x_0} + \norm{x_0 - x} \l \delta^\prime + \delta.\]
    Also,
    \[\norm{T(x_1 - x)} = \norm{T(x_1 - x_0 + x_0 - x)} \leq \norm{T(x_1 - x_0)} + \norm{T(x_0 - x)} \l c\norm{x_1 - x_0} + \epsilon = c\delta^\prime + \epsilon. \]
    Thus, continuity at any other point is implied.
\end{proof}

\begin{question}
    Show that a operator $T : \ell^\infty \mapsto \ell^\infty$ defined by $y = (\eta_j) = Tx, \eta_j = \xi_j/j,x = (\xi_j)$ is linear and bounded.
    \label{section2.7-5}
\end{question}
\begin{proof}
    First, we have
    \[(T(\alpha x_1 + \beta x_2))_j = \frac{\alpha \xi^{(1)}_j + \beta \xi^{(2)}_j}{j} = \alpha\frac{\xi^{(1)}_j}{j} + \beta \frac{\xi^{(2)})j}{j} = \alpha (Tx_1)_j + \beta (Tx_2)_j.\]
    Thus, $T$ is linear. Now, the norm in the $\ell^\infty$ space is defined as
    \[\norm{T} = \sup_{\norm{x} = 1} \norm{Tx}_\infty = \sup_{\norm{x} = 1} \sup_{j} \abs{(Tx)_j}.\]
    Let $x$ be any sequence such that $\norm{x}_\infty = 1$. In other words,
    \[\sup_{j} \abs{\xi_j} = 1 \implies \abs{\xi_j} \leq 1.\]
    Then, 
    \[\sup_{j} \abs{(Tx)_j} = \sup_{j} \abs{\frac{\xi_j}{j}} \leq \sup_{j} \abs{\xi_j}  = 1.\]
    Thus, $T$ is also bounded.
\end{proof}

\begin{question}
    Show that the range $\range{T}$ of a bounded linear operator $T : X \mapsto Y$ need not be closed in $Y$.
    \label{section2.7-6}
\end{question}
\begin{proof}
    Consider the example from \ref{section2.7-5}:
    \[y = Tx = (\eta_j) , \eta_j = \xi_j / j , x = (\xi_j).\]
    where $x , y \in \ell^\infty$. Let \[x_n = \left( 1 , \frac{1}{2} , \ldots , \frac{1}{n} , 0 \ldots, 0\right) \implies Tx_n = \left( 1 , \frac{1}{4} , \ldots , \frac{1}{n^2} , 0 \ldots, 0\right).\]
    In such a case,
    \[Tx_n \rightarrow \left( 1 , \frac{1}{4} , \ldots , \frac{1}{n^2} , \frac{1}{(n+1)^2} \ldots \right),\]
    however, the corresponding element from $\ell^\infty$ to produce the limit point would be
    \[\left( 1 , \frac{1}{2} , \ldots , \frac{1}{n} , \frac{1}{n+1} \ldots, \right) \notin \ell^\infty.\]
    Thus, the limit point does not belong to the range and hence, the range is not closed.
\end{proof}

\begin{question}
    Let $T$ be a bounded linear operator from a normed space $X$ onto a normed space $Y$. If there is a positive $b$ such that 
    \[\norm{Tx} \geq b \norm{x} \;\forall x \in X\]
    then show that $T^{-1} : Y \mapsto X$ exists and is bounded.
    \label{section2.7-7}
\end{question}
\begin{proof}
    To show that $T^{-1}$ exists, we show that the null space only consists of the zero element. Suppose not. Then, let $Tx = 0 , x \neq 0$. Then,
    \[\norm{Tx} = 0 \geq b \norm{x} \geq 0 \implies \norm{x} = 0 \implies x = 0\]
    which shows that $Tx= 0 \implies x = 0$. Thus, $T^{-1}$ exists. 

    Now, to show $T^{-1}$ is bounded, note that
    \[b\norm{x} \leq \norm{Tx} \leq c \norm{x}.\]
    Let $y = Tx$, then
    \[\norm{T^{-1}y} = \norm{x} \leq \frac{1}{b}\norm{y}\]
    and hence, $T^{-1}$ is bounded.
\end{proof}

\begin{question}
    Show that the inverse $T^{-1} : \range{T} \mapsto X$ of a bounded linear operator $T : X \mapsto Y$ need not be bounded.
    \label{section2.7-8}
\end{question}
\begin{proof}
    Consider the example in \ref{section2.7-5},
    \[y = (\eta_j) = Tx, \eta_j = \xi_j/j , x = (\xi_j).\]
    Clearly, in this case, $T^{-1}y = (j\eta_j)$. Suppose $y \in \ell^\infty$ such that $\norm{y}_\infty = 1$. Then,
    \[\norm{y}_\infty = 1 \implies \sup_{j}\abs{\eta_j} = 1 \implies \abs{\eta_j} \leq 1.\]
    Now, consider
    \[\norm{T^{-1}y} = \sup_{\norm{y} = 1} \sup_{j} \norm{\eta_j j} \leq \sup_j \abs{\eta_j} j \leq \sup_j j\]
    which is clearly unbounded. Thus, the inverse mapping need not be bounded.
\end{proof}

\begin{question}
    Let $T : C[0,1] \mapsto C[0,1]$ such that
    \[y(t) = \int_{0}^t x(\tau) \;d\tau.\]
    Find $\range{T}$ and $T^{-1} : \range{T} \mapsto C[0,1]$. Is $T^{-1}$ linear and bounded?
    \label{section2.7-9}
\end{question}
\begin{proof}
    The range of $T$, $\range{T}$ is the subspace of all continuous functions on $[0,1]$ such that $y(0) = 0.$ Clearly, $T^{-1}y(t) = y^\prime(t)$. $T^{-1}$ is linear since differentiation is a linear operation. However, it is not bounded. Consider $y(t) = t^n$. Then, 
    \[\norm{y} = \max_{t \in [0,1]} \abs{y(t)} = 1.\]
    Now, 
    \[\norm{T^{-1}y} = \max_{t \in [0,1]} \abs{nt^{n-1}} = n = n\norm{y}\]
    and hence, $\norm{T^{-1}} = n$ which is unbounded.
\end{proof}

\begin{question}
    On $C[0,1]$, define $S$ and $T$ by
    \[y(s) = s\int_{0}^{1}x(t) \; dt \;,\; y(s) = sx(s).\]
    Do $S$ and $T$ commute? Find $\norm{S} , \norm{T} , \norm{ST}, \norm{TS}$.
    \label{section2.7-10}
\end{question}
\begin{proof}
    Consider 
    \[STx(t) = Stx(t) = t\int_{0}^1 tx(t) \; dt = t^2 \int_{0}^{1}x(t) \; dt.\]
    On the other hand, 
    \[TSx(t) = Tt\int_{0}^{1} x(t) \;dt = t^2 \int_{0} {1} x(t) \;dt.\]
    Thus, $S$ and $T$ commute. Now, let $\norm{x} = 1.$ Thus, 
    \[\norm{x} = \max_{t \in [0,1]} \abs{x(t)} = 1 \implies \abs{x(t)} \leq 1.\]
    Now, 
    \[\norm{S} = \sup_{\norm{x} = 1} \norm{Sx} = \sup_{\norm{x} = 1} \max_{t \in [0,1]} t\int_{0}^1 x(t) \;dt \leq 1 \cdot \int_{0}^{1} 1\;dt = 1\]
    \[\norm{T} = \sup_{\norm{x} = 1} \norm{Tx} = \sup_{\norm{x} = 1} \max_{t \in [0,1]} tx(t) \leq 1 \cdot 1 = 1\]
    \[\norm{ST} = \norm{TS} = \sup_{\norm{x} = 1} \norm{STx} = \sup_{\norm{x} = 1} \max_{t \in [0,1]} t^2\int_{0}^1 x(t) \;dt \leq 1 \cdot \int_{0}^{1} 1\;dt = 1\]
   
\end{proof}

\begin{question}
    Let $X$ be the normed space of all bounded real-valued functions on $\R$ with norm defined by
    \[\norm{x} = \sup_{t \in \R} \abs{x(t)}\]
    and let $T : X \mapsto X$ be defined by
    \[y(t) = Tx(t) = x(t - \Delta).\]
    Here, $\Delta \g 0$ is a constant. Is $T$ linear and bounded?
    \label{section2.7-11}
\end{question}
\begin{proof}
    First, to check if $T$ is linear, we have
    \[T(\alpha x_1 + \beta x_2)(t) = (\alpha x_1 + \beta x_2)(t - \Delta) = \alpha x_1(t-\Delta) + \beta x_2(t-\Delta) = \alpha Tx_1(t) + \beta Tx_2(t). \]
    where the last inequality follows since $x_1$ and $x_2$ are bounded real-valued functions. Thus, $T$ is linear.
    Now, let $\norm{x} = 1$.
    \[\norm{x} = \max_{t \in R}\abs{t} = 1 \implies \abs{x(t)} \leq 1 \; \forall t \in \R.\]
    Thus, 
    \[\norm{T} = \sup_{\norm{x} = 1} \norm{Tx} = \max_{t \in \R} \abs{x(t - \Delta)} \leq 1.\]
    since $t \in \R \implies t - \Delta \in R$ since $\Delta \g 0$ is fixed. Thus, $T$ is bounded.
\end{proof}

\begin{question}
    From 2.7-7, we know that an $r \times n$ matrix $A = (a_{jk})$ defines a linear operator from the vector space $X$ of all ordered $n-$tuples of numbers into the vector soace $Y$ of all ordered $r-$tuples of numbers. Suppose that any norm $\norm{.}_1$ is given on $X$ and any norm $\norm{.}_2$ is given on $Y$. there could be various norms on the space $Z$ of all matrices. A norm $\norm{.}$ on $Z$ is said to be compatible with $\norm{.}_1$ and $\norm{.}_2$ if 
    \[\norm{Ax}_2 \leq \norm{A}\norm{x}_1.\]
    Show that the norm defined by
    \[\norm{A} = \sup_{\substack{x \in X \\ x \neq 0}} \frac{\norm{Ax}_2}{\norm{x}_1}\]
    is compatible with $\norm{.}_1$ and $\norm{.}_2$. This norm is often called the natural norm defined by $\norm{.}_1$ and $\norm{.}_2$. If we choose $\norm{x}_1 = \max_{j} \abs{\xi_j}$ and $\norm{y}_2 = \max_i \abs{\eta_i}$, show that the natural norm is 
    \[\norm{A} = \max_j \sum_{k=1}^n \abs{a_{jk}}.\]
    \label{section2.7-12}
\end{question}
\begin{proof}
    First, the norm
    \[\norm{A} = \sup_{\substack{x \in X \\ x \neq 0}} \frac{\norm{Ax}_2}{\norm{x}_1}\]
    is always non-negative. Let $\norm{A} = c$, then for all $x \in X$ such that $x \neq 0$, we have
    \[\frac{\norm{Ax}_2}{\norm{x}_1} \leq c \implies \norm{Ax}_2 \leq c \norm{x}_1 \implies \norm{Ax}_2 \leq \norm{A} \norm{x}_1.\]
    Thus, the given norm is a natural norm for $\norm{.}_1$ and $\norm{.}_2$.

    Suppose, we now consider the norms
    \[\norm{x}_1 = \max_{j} \abs{\xi_j} \;,\; \norm{y}_2 = \max_{i} \abs{\eta_i}.\]
    Then, we have
    \begin{align*}
        \norm{y}_2 &= \max_{j \in [r]} \abs{\sum_{k=1}^n a_{jk} \xi_k} \leq \max_{j \in [r]} \sum_{k=1}^n \abs{a_{jk}} \abs{\xi_k} \leq \max_{j \in [r]} \sum_{k=1}^n \abs{a_{jk}} \left(\max_{i 
        \in [n]} \xi_i \right) = \norm{x}_1 \max_{j \in [r]} \sum_{k=1}^n \abs{a_{jk}}
    \end{align*}
    and hence, $\norm{A} = \max_{j \in [r]} \sum_{k=1}^n \abs{a_{jk}}$.
\end{proof}

\begin{question}
    Show that if $A \in \R^{n \times n}$, then a compatible norm is defined by
    \[\norm{A} = \left(\sum_{j=1}^n \sum_{k=1}^n a_{jk}^2 \right)^{1/2}\]
    but for $n \g 1$ this is not the natural norm defined by the Euclidian norm on $\R^n$.
    \label{section2.7-13}
\end{question}
\begin{proof}
    Let $y  =Ax$. Then, 
    \begin{align*}
        \norm{y} &= \left(\sum_{i=1}^n \left( \sum_{j=1}^n a_{ij}x_j \right)^2\right)^{1/2}
        \\
        &\leq \left(\sum_{i=1}^n \left( \sqrt{\sum_{j=1}^n a_{ij}^2} \sqrt{\sum_{j=1}^n x_j^2} \right)^2\right)^{1/2}
        \\
        &= \left(\norm{x}^2 \sum_{i=1}^n \sum_{j=1}^n a_{ij}^2 \right)^{1/2}
        \\
        &= \norm{x}\left( \sum_{i=1}^n \sum_{j=1}^n a_{ij}^2 \right)^{1/2}
    \end{align*}
    where the first inequality follows from HÃ¶lder's inequality, which gives us
    \[\sum \abs{x_i y_i} \leq (\sum \abs{x_i}^p)^{1/p} (\sum \abs{y_i}^q)^{1/q} \;,\; p^{-1} + q^{-1} = 1. \]
    Thus, this norm, also called the Frobenius norm is compatible with the Euclidian norm. Now, we shall find the natural norm. Recall, the natural norm is defined as
    \[\norm{A} = \sup_{\substack{x \in X \\ x \neq 0}} \frac{\norm{Ax}}{\norm{x}}.\]
    Let $A = I$. Then, the Frobenius norm results in $\norm{A} = \sqrt{n}$, while the natural norm results in $\norm{A} = 1$. Thus, these are not the same norms, unless $n = 1$.
\end{proof}

\begin{question}
    If in \ref{section2.7-12}, we choose
    \[\norm{x}_1 = \sum_{k=1}^n \abs{\xi_k} \;,\; \norm{y}_2 = \sum_{j=1}^r \abs{a_{jk}},\]
    then show that a compatible norm is defined by
    \[\norm{A} = \max_{k} \sum_{j=1}^r \abs{a_{jk}}.\]
    \label{section2.7-14}
\end{question}
\begin{proof}
    We have,
    \begin{align*}
        \norm{y} &= \sum_{j=1}^r \abs{\eta_j} = \sum_{j=1}^r \abs{\sum_{k=1}^n a_{jk}\xi_k}
        \\
        &\leq \sum_{j=1}^r \sum_{k=1}^n \abs{a_{jk}}\abs{\xi_k}
        \\
        &\leq \sum_{j=1}^r \sum_{k=1}^n \left(\max_{k \in n}\abs{a_{jk}}\right) \abs{\xi_k}
        \\
        &= \sum_{j=1}^r \left(\max_{k \in n}\abs{a_{jk}}\right) \norm{x}_1
        \\
    \end{align*}
    and hence, we get a compatible norm as 
    \[\norm{A} = \sum_{j=1}^r \left(\max_{k \in n}\abs{a_{jk}}\right).\]
\end{proof}

\begin{question}
    Show that for $r = n$, the norm in \ref{section2.7-14} is the natural norm corresponding to $\norm{.}_1$ and $\norm{.}_2$ as defined in the problem.
    \label{section2.7-15}
\end{question}
\begin{proof}
    The norm is defined as 
    \[\norm{A} = \max_{k \in [n]} \sum_{j=1}^n \abs{a_{jk}}\]
    From \ref{section2.7-14}, we had
    \[\norm{Ax}_2 \leq \norm{A} \norm{x} \implies \norm{A} \geq \frac{\norm{Ax}_2}{\norm{x}_1} \;\forall x \neq 0 \implies \norm{A} = \sup_{x \neq 0} \frac{\norm{Ax}_2}{\norm{x}_1}\]
    which is exactly the definition of the natural norm.
\end{proof}