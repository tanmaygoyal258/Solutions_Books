\subsection{Vector Space}

\begin{question}
    Show that the set of all real numbers with the usual addition and multiplication constitutes a one-dimensional real vector space and the set of all complex numbers constitutes a one-dimensional complex vector space.
    \label{section2.1-1}
\end{question}

\begin{proof}
    In the set of all real numbers, it is easy to see that that addition satisfies associativity, commutativity, and consists of an identity $(0)$, and an inverse $(-x) \in \R$. Also, multiplication satisfies associativity, has an identity $(1)$, and satisfies distributivity. Thus, it is a vector space. 

    For the set of complex numbers, addition satisfies associativity, commutativity, and consists of an identity ($0 + \iota 0$), and an inverse $(-x) \in \C$. Also, multiplication satisfies associativity, has an identity $(1 + \iota 0)$, and satisfies distributivity. Thus, it is a vector space. 
\end{proof}

\begin{question}
    Prove (1) and (2).
    \label{section2.1-2}
\end{question}
\begin{proof}
    Denote the zero vector as $\theta.$ Then, we wish to prove:
    \[0x = \theta\]
    \[\alpha \theta = \theta\]
    \[(-1)x = -x\]
    For the first statement, notice that for some $\alpha \in K$ (where $K$ is the scalar field), we have
    \[\theta = \alpha x + (-\alpha) x = (\alpha + (-\alpha)x
    ) = 0x \]
    where the first equality follows from the properties of addition on the vector space, the second equality follows from the distributive property for vector spaces, and the third follows from the properties of addition on $K$ (there exists an additive inverse).

    For the second statement, substituting $x = \theta$ in the first statement gives us the result.

    For the third statement, we have
    \[(1 + (-1))x = \theta \implies (-1)x = -x\]
    where the implication follows from distributive property.
\end{proof}

\begin{question}
    Describe the span of $M = \{(1,1,1) , (0,0,2)\} \in \R^3$.
    \label{section2.1-3}
\end{question}
\begin{proof}
    The span of $M$ is described as all possible linear combinations of the vectors in $M$. Let the coefficients be $\alpha \in \R$ and $\beta \in \R$ for $(1,1,1)$ and $(0,0,2)$ respectively. Then, we have
    \[\textrm{span}\; M = \{(\alpha , \alpha , \alpha + 2\beta)\}\]
    which represents a plane passing through the line $x=y$.
\end{proof}

\begin{question}
    Which of the following subsets of $\R^3$ constitute a subspace of $\R^3$? Here, $x = (\xi_1 ,\xi_2 , \xi_3)$.
    \begin{enumerate}
        \item All $x$ with $\xi_1 = \xi_2$ and $\xi_3 = 0$?
        \item All $x$ with $\xi_1 = \xi_2 + 1$.
        \item All $x$ with positive $\xi_1$, $\xi_2,\xi_3$.
        \item All $x$ with $\xi_1 - \xi_2 + \xi_3 = k$.
    \end{enumerate}
    \label{section2.1-4}
\end{question}
\begin{proof}
    Denote the subsets by $X$.
    \begin{enumerate}
        \item Consider two points $x_1 = (a_1,a_1,0) \in X$ and $x_2 = (a_2,a_2,0) \in X$. Then, we have $x_1 + x_2 = (a_1 + a_2 , a_1 + a_2 , 0) \in X$. Also, let $\alpha$ be some scalar, then, $\alpha x_1 = (\alpha a_1 , \alpha a_1 , 0) \in X$. Hence, $X$ is a subspace.
        \item Consider two points $x_1 = (1 + a_1 , a_1 , b_1) \in X$ and $x_2 = (1 + a_2 , a_2 , b_2) \in X$. Then, $x_1 + x_2 = (2 + a_1 + a_2 , a_1 + a_2 , b_1 + b_2) \notin X$ since $2 + a_1 + a_2 \neq 1 + (a_1 + a_2)$. Thus, $X$ is not a subspace.
        \item Let $x_1 = (a_1 , b_1 , c_1) \in X$, i.e, $a_1 , b_1 , c_1 \g 0$. Consider some $\alpha \l 0$, then, $\alpha x_1 \notin X$, since the coordinates now become negative. Hence, $X$ is not a subspace.
        \item Consider $x_1 = (a_1 , b_1 , c_1) \in X$ and let $\alpha$ be some scalar. Then, $\alpha x_1 = (\alpha a_1 , \alpha b_1 , \alpha c_1)$. However, $\alpha(a_1 -b_1 + c_1) \neq k$ and hence, $X$ is not a subspace.
    \end{enumerate}
\end{proof}

\begin{question}
    Show that $\{x_1 , \ldots , x_n\}$ where $x_j(t) = t^j$ is a linearly independent set in the space $C[a,b]$.
    \label{section2.1-5}
\end{question}
\begin{proof}
    We wish to show that 
    \[\sum_{j=1}^n \alpha_j x_j = 0 \implies \alpha_j = 0 \; \forall j\in [n]. \]
    Note that this convergence has to be pointwise. Consider the function $\sum_{j=1}^n \alpha_j x_j$. Then,
    \[\left(\sum_{j=1}^n \alpha_j x_j \right)(t) = \sum_{j=1}^n \alpha_j t^j  = 0\]
    Clearly, if $t \g 0$, then this only holds if all the coefficients $\alpha_j = 0.$
\end{proof}

\begin{question}
    Show that in a $n-$dimensional vector space $X$, the representation of any $x$ as a linear combination of given basis vector is unique.
    \label{section2.1-6}
\end{question}
\begin{proof}
    Suppose not. Then, let the two sets of coefficients be $\{\alpha_i\}$ and $\{\beta_i\}$. We have that
    \[x = \sum_{i=1}^n \alpha_i e_i = \sum_{i=1}^n \beta_i e_i.\]
    Thus, we also have that
    \[\sum_{i=1}^n (\alpha_i - \beta_i) e_i = 0.\]
    But since the set of vectors $e_i$ are linearly independent, this means that for all $i$, $\alpha_i - \beta_i = 0 \implies \alpha_i  =\beta_i$. This finishes the proof.
\end{proof}

\begin{question}
    Let $\{e_1 , \ldots , e_n\}$ be a basis for a complex vector space $X$. Find a basis for $X$ regarded as a real vector space. What is the dimension of $X$ in either case?
    \label{section2.1-7}
\end{question}
\begin{proof}
    Suppose $X$ were a real space. Then, we would need a separate basis for the real parts and the imaginary parts. This leads to the basis of $\{e_1 , \ldots , e_n , \iota e_1 , \ldots , \iota e_n\}$, resulting in a dimension of $2n$.
\end{proof}

\begin{question}
    If $M$ is linearly dependent set in a complex vector space $X$, is $M$ linearly dependent in $X$, regarded as a real vector space?
    \label{section2.1-8}
\end{question}
\begin{proof}
    Suppose $M = \{a_i + \iota b_i\}_{i=1}^n$ and let 
    \[a_i + \iota b_r = \sum_{\substack{i=1}\\i\neq r}^n \frac{\alpha_i}{\alpha_r} (a_i + \iota b_i)\]
    Equating the real and imaginary parts results in 
    \[\alpha_r = \sum_{\substack{i=1}\\i\neq r}^n {\alpha_r} a_i \text{ and } b_r = \sum_{\substack{i=1}\\i\neq r}^n {\alpha_r} b_i.\]
    Thus, regarding $X$ as real vector space, each of the coefficients can still be expressed as a linear combination of the other vectors. Thus, it is still linearly dependent.
\end{proof}

\begin{question}
    On a fixed interval $[a,b] \subset \R$, consider the set $X$ consisting of all polynomials with real coefficients and of degree not exceeding a given $n$ and the polynomial $x = 0$. Show that $X$ with the usual addition and multiplication by real numbers is a real vector space of dimension $n+1.$ Find a basis for $X$. Show that we can obtain a complex vector space $\overline{X}$ in a similar fashion if we let those coefficients be complex. Is $X$ a subspace of $\overline{X}$?
    \label{section2.1-9}
\end{question}
\begin{proof}
    First, note that for addition, associativity and commutativity holds. Also, the zero polynomial is a part of $X$, and hence, there exists an additive identity. Further, the additive inverse is simply given by negating all coefficients, and hence, the additive inverse also belongs to the set $X$. Similarly, the associative property for multiplication also holds, and a multiplicative identity is simply the polynomial $x = 1.$ Thus, it is a valid vector space. If every polynomial has degree at most $n$, then we require the monomials $x^n , x^{n-1} \ldots , x , 1$ to build each polynomial, and hence, the degree is $n+1$. The basis is the set of monomials $\{x^j\}_{j=0}^n$. If each of the coefficients are complex, then the overall polynomial can be split into a real polynomial and an imaginary polynomial, resulting in the space of complex polynomials. However, $X$ is not a subspace of $\overline{X}$ because allowing complex coefficients with polynomials of $X$ would result in a complex polynomial, which will not belong to $X$.
\end{proof}

\begin{question}
    Show that if $Y$ and $Z$ are subspaces of a vector space $X$, show that $Y \cap Z$ is a subspace of $X$ but $Y \cup Z$ need not be one.
    \label{section2.1-10}
\end{question}
\begin{proof}
    Let the set of vectors in $Y \cap Z$ be denoted by $u_i$. Then, by the definition of $u_i \in Y$ and $u_i \in Z$. Then, any linear combination $\sum \alpha_j u_j$ will belong to both $Y$ and $Z$ and hence, to $Y \cap Z$. Thus, $Y \cap Z$ is a subspace. On the other hand, the union of subspaces may not be a subspace. Let $Y$ be the subspace of polynomials of the form $\alpha + \beta x$, while let $Y$ is the subspace of polynomials of the form $\alpha + \beta x^2$. Then, the union is simply all polynomials of the form $\alpha + \beta x$ or $\alpha + \beta x^2$. However, a linear combination of the vectors in $Y \cup Z$ may result in polynomials of the form $\alpha + \beta x + \gamma x^2$, which do not belong to $Y \cup Z.$
\end{proof}

\begin{question}
    If $M \neq \emptyset$ is any subset of a vector space $X$, show that $\textrm{span} \; M$ is a subspace of $X$.
    \label{section2.1-11}
\end{question}
\begin{proof}
    Let $M = \{x_1 , \ldots , x_n\}$. By the definition of span, we know that
    \[\textrm{span}\; M = \{\sum_{j=1}^n \alpha_j x_j \mid (\alpha_1 , \ldots , \alpha_n) \in \R^n\}.\]
    Suppose we take two vectors $\sum_{j=1}^n \alpha_j x_j \in \textrm{span}\; M$ and $\sum_{j=1}^n \beta_j x_j \in \textrm{span}\; M$. Representing $\gamma_j = \alpha_j + \beta_j$, we get the sum of the vectors to be $\sum_{j=1}^n \gamma_j x_j$, which belongs to $M$. Similarly, let $\alpha_j \beta = \gamma_j$, then multiplying the vector $\sum_{j=1}^n \alpha_j x_j$ by $\beta$ results in the vector $\sum_{j=1}^n \gamma_j x_j \in \textrm{span}\; M$. Thus, $M$ is a subspace.
\end{proof}

\begin{question}
    Show that the set of all real two rowed square matrices forms a vector space $X$. What is the zero vector in $X$? Determine $\dim X$. Find a basis for $X$. Give examples of subspaces for $X$. Do the symmetric matrices $x \in X$ form a subspace? The singular matrices?
    \label{section2.1-12}
\end{question}
\begin{proof}
    Clearly, addition for the two-rowed square matrices is associative and commutative, since the addition is element-wise. Further, the additive identity can be represented as
    \[\mathbbm{0} = 
    \begin{pmatrix}
        0 & 0\\ 0 & 0
    \end{pmatrix}
    \]
    Also, the additive inverse can be defined as follows:
    \[x = \begin{pmatrix}a & b \\ c & d\end{pmatrix} \implies -x = \begin{pmatrix} -a & -b \\ -c & -d\end{pmatrix}\]
    Also, multiplication is associative (and not commutative!), and we can define the multiplicative identity as
    \[\mathbbm{1} = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}\]
    Thus, this space is a vector space. Clearly, $\dim X = 4$ and the basis is the following set:
    \[\left\{\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} , \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0\\0 & 1 \end{pmatrix}\right\}\]
    The set of symmetric matrices is a subspace because the addition of two symmetric matrices is also symmetric, while multiplying by a constant maintains symmetry. However, the set of all singular matrices is not a subspace. Denote the set of singular matrices by $S$. Then,
    \[\begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix} \in S \text{ and } \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} \in S \text{ but } \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix} + \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} =  \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \notin S.\]
\end{proof}

\begin{question}
    Show that the Cartesian product $X = X_1 \times X_2$ of two vector spaces over the same field becomes a vector space if we define the two algebraic operations by
    \[(x_1 , x_2) + (y_1 , y_2) = (x_1 + x_2 , y_1 + y_2),\]
    \[\alpha(x_1 ,x_2) = (\alpha x_1 , \alpha x_2)\].
    \label{section2.1-13}
\end{question}
\begin{proof}
    We first have
    \[(x_1 , x_2) + (y_1 , y_2) = (x_1 + y_1 , x_2 + y_2) = (y_1 , y_2) + (x_1 ,x_2)\]
    \[((x_1 , x_2) + (y_1 , y_2)) + (z_1 , z_2) = (x_1 + y_1 + z_1 , x_2 + y_2 + z_2) = (x_1 , x_2) + ((y_1 , y_2) + (z_1 , z_2))\]
    \[(x_1 + x_2) + (-x_1 , -x_2) = (0,0)\]
    and hence, all properties of addition are satisfied.
    Similarly, we can verify for scalar multiplication:
    \[\alpha(\beta(x_1,x_2)) = \alpha(\beta x_1 , \beta x_2) = (\alpha\beta x_1 , \alpha \beta x_2) = \beta(\alpha x_1 , \alpha x_2) = \beta (\alpha x)\]
    \[1(x_1 , x_2) = (x_1 , x_2)\]
    and can verify the distributive laws. Thus, this space is a vector space.
\end{proof}

\begin{question}
    Let $Y$ be a subspace of a vector space $X$. The coset of an element $x \in X$ with respect to $Y$ is deonted by $x + Y$ and is defined to be the set
    \[x + Y = \{v \mid v = x + y , y \in Y\}.\]
    Show that the distinct cosets form a partition of $X$. Show that under algebraic operations defined by 
    \[(w+Y) + (x+Y) = (w + x) + Y\]
    \[\alpha(x + Y) = \alpha x + Y\]
    these cosets constitute the elements of a vector space. This space is called the quotient space of $X$ by $Y$ and is denoted by $X/ Y$. It's dimension is called the co-dimension of $Y$ and is denoted by $\textrm{codim}\; Y = \dim (X/Y)$.
    \label{section2.1-14}
\end{question}
\begin{proof}
   We show that no two cosets can have the same element, and if they do, then they are essentially the same coset. Let $a \in X$ be some element which belongs to two different cosets: $x + Y$ and $w + Y$. Thus, there exists $y_x \in Y$ and $y_w \in Y$ such that
   \[a = x + y_x = w + y_w \implies x = a - y_x \text{ and } w = a - y_w.\]
   Thus, we can write the cosets as
   \[x + Y = \{a - y_x + y \mid y \in Y\}\]
   \[w + Y = \{a + y_w - y \mid y \in Y\}.\]
    Since $Y$ is a subspace, $-y_x + y$ for all $y \in Y$ will always result in some vector belonging to $y$, and similarly, $-y_w + y$ will always result in some vector belonging to $Y$. Thus, as we circle through all elements of $Y$, the sets $\{y-_x + y \mid y \in Y\}$ and $\{-y_w + y \mid y \in Y\}$ would be equal sets. Hence, the cosets $x + Y$ and $w + Y$ would be equal. Also, since $X$ is a vector space, for any $x \in X$ and $y \in Y$, $x - y$ has to belong to $X$ and hence, to one of the cosets. This shows that the distinct cosets form partitions of $X$.

    Consider the sets :
    \[x + Y = \{x + y \mid y \in Y\},\]
    \[w + Y = \{w + y \mid y \in Y\}.\]
    Then, we have
    \[(w + Y) + (x + Y) = \{w + y + x + y \mid y \in Y\},\]
    but since $2y \in Y$ (since Y is a subspace), denoting $2y = y^\prime$, we have
    \[(w + Y) + (x + Y) = \{w + x + y^\prime \mid y^\prime \in Y\} = (w+x) + Y.\]
    Similarly, 
    \[\alpha(x + Y) = \{\alpha(x + y) \mid y \in Y\}.\]
    However, since $\alpha y \in Y$ (since $Y$ is a subspace), denoting $\alpha y = y^\prime$, we have
    \[\alpha(x + Y) = \{\alpha x + y^\prime \mid y^\prime \in Y\} = \alpha x + Y.\]
    This finishes the proof.
\end{proof}

\begin{question}
    Let $X = \R^3$ and $Y = \{(\xi_1 , 0 , 0) \mid \xi_1 \in \R\}$. Find $X / Y$, $X / X$, and $X / \{0\}$.
    \label{section2.1-15}
\end{question}
\begin{proof}
    Note that $X/Y = \{\{(\xi_1 , \xi_2 , \xi_3) \mid \xi_2 , \xi_3 \in \R\} \mid  \xi_1 \in \R\}$. This means that each corset is obtained by fixing the $\xi_1$ coordinate and varying $\xi_2$ and $\xi_3$. Thus, each of the cosets is a plane parallel to $\xi_1$ axis. Now, $X / X$ is simply $\{0\}$ because for any $x \in X$, the coset $x + X  = \{x + x_0 \mid x_0 \in X\}$ is simply $X$ and hence, there is no space of distinct cosets, i.e this quotient space is empty. Finally, $X / \{0\}$ is simply $X$ since each point in itself would be a coset, and the space of these cosets is $X$ itself.
\end{proof}